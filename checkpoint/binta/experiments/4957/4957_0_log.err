Downloading: "https://download.pytorch.org/models/resnet101-cd907fc2.pth" to /root/.cache/torch/hub/checkpoints/resnet101-cd907fc2.pth
  0%|          | 0.00/171M [00:00<?, ?B/s]  4%|▍         | 7.57M/171M [00:00<00:02, 79.4MB/s] 10%|▉         | 16.7M/171M [00:00<00:01, 89.1MB/s] 16%|█▌        | 27.2M/171M [00:00<00:01, 98.7MB/s] 22%|██▏       | 38.0M/171M [00:00<00:01, 104MB/s]  28%|██▊       | 48.0M/171M [00:00<00:01, 102MB/s] 34%|███▍      | 58.0M/171M [00:00<00:01, 103MB/s] 40%|███▉      | 67.8M/171M [00:00<00:01, 102MB/s] 46%|████▌     | 78.8M/171M [00:00<00:00, 106MB/s] 53%|█████▎    | 89.6M/171M [00:00<00:00, 108MB/s] 60%|██████    | 102M/171M [00:01<00:00, 116MB/s]  67%|██████▋   | 114M/171M [00:01<00:00, 117MB/s] 75%|███████▍  | 127M/171M [00:01<00:00, 124MB/s] 82%|████████▏ | 139M/171M [00:01<00:00, 125MB/s] 91%|█████████ | 155M/171M [00:01<00:00, 136MB/s] 99%|█████████▉| 169M/171M [00:01<00:00, 142MB/s]100%|██████████| 171M/171M [00:01<00:00, 118MB/s]
tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<00:00, 128kB/s]
vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 7.37MB/s]vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 7.29MB/s]
merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 7.71MB/s]
tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.16MB/s]tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 6.11MB/s]
config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]config.json: 100%|██████████| 481/481 [00:00<00:00, 2.77MB/s]
model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]model.safetensors:   4%|▍         | 21.0M/499M [00:00<00:02, 167MB/s]model.safetensors:  11%|█         | 52.4M/499M [00:00<00:02, 222MB/s]model.safetensors:  17%|█▋        | 83.9M/499M [00:00<00:01, 239MB/s]model.safetensors:  23%|██▎       | 115M/499M [00:00<00:01, 248MB/s] model.safetensors:  29%|██▉       | 147M/499M [00:00<00:01, 237MB/s]model.safetensors:  36%|███▌      | 178M/499M [00:00<00:01, 231MB/s]model.safetensors:  42%|████▏     | 210M/499M [00:00<00:01, 219MB/s]model.safetensors:  48%|████▊     | 241M/499M [00:01<00:01, 219MB/s]model.safetensors:  55%|█████▍    | 273M/499M [00:01<00:01, 219MB/s]model.safetensors:  61%|██████    | 304M/499M [00:01<00:00, 225MB/s]model.safetensors:  67%|██████▋   | 336M/499M [00:01<00:00, 225MB/s]model.safetensors:  74%|███████▎  | 367M/499M [00:01<00:00, 234MB/s]model.safetensors:  80%|███████▉  | 398M/499M [00:01<00:00, 235MB/s]model.safetensors:  86%|████████▌ | 430M/499M [00:01<00:00, 239MB/s]model.safetensors:  92%|█████████▏| 461M/499M [00:02<00:00, 234MB/s]model.safetensors:  99%|█████████▉| 493M/499M [00:02<00:00, 237MB/s]model.safetensors: 100%|██████████| 499M/499M [00:02<00:00, 229MB/s]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
submitit ERROR (2024-05-10 13:39:09,495) - Submitted job triggered an exception
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/submitit/core/_submit.py", line 11, in <module>
    submitit_main()
  File "/usr/local/lib/python3.10/dist-packages/submitit/core/submission.py", line 76, in submitit_main
    process_job(args.folder)
  File "/usr/local/lib/python3.10/dist-packages/submitit/core/submission.py", line 69, in process_job
    raise error
  File "/usr/local/lib/python3.10/dist-packages/submitit/core/submission.py", line 55, in process_job
    result = delayed.result()
  File "/usr/local/lib/python3.10/dist-packages/submitit/core/utils.py", line 133, in result
    self._result = self.function(*self.args, **self.kwargs)
  File "/content/drive/.shortcut-targets-by-id/1AH0JByxrzaqnJ9Z6_OpI2C4rxuUEK3UM/mdetr/run_with_submitit.py", line 100, in __call__
    detection.main(self.args)
  File "/content/drive/.shortcut-targets-by-id/1AH0JByxrzaqnJ9Z6_OpI2C4rxuUEK3UM/mdetr/main.py", line 360, in main
    [build_dataset(name, image_set="train", args=args) for name in args.combine_datasets]
  File "/content/drive/.shortcut-targets-by-id/1AH0JByxrzaqnJ9Z6_OpI2C4rxuUEK3UM/mdetr/main.py", line 360, in <listcomp>
    [build_dataset(name, image_set="train", args=args) for name in args.combine_datasets]
  File "/content/drive/.shortcut-targets-by-id/1AH0JByxrzaqnJ9Z6_OpI2C4rxuUEK3UM/mdetr/datasets/__init__.py", line 51, in build_dataset
    return build_refexp(image_set, args)
  File "/content/drive/.shortcut-targets-by-id/1AH0JByxrzaqnJ9Z6_OpI2C4rxuUEK3UM/mdetr/datasets/refexp.py", line 109, in build
    dataset = RefExpDetection(
  File "/content/drive/.shortcut-targets-by-id/1AH0JByxrzaqnJ9Z6_OpI2C4rxuUEK3UM/mdetr/datasets/coco.py", line 20, in __init__
    super(ModulatedDetection, self).__init__(img_folder, ann_file)
  File "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/coco.py", line 36, in __init__
    self.coco = COCO(annFile)
  File "/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py", line 84, in __init__
    dataset = json.load(open(annotation_file, 'r'))
FileNotFoundError: [Errno 2] No such file or directory: 'mdetr_annotations/final_refexp_train.json'
